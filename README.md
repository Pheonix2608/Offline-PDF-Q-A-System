# ðŸ§  Offline AI-Powered Document Assistant

> **An offline chatbot for querying documents using local LLMs via Ollama, LangChain, FAISS & Gradio.**  
> Fully private. No cloud. Runs locally. Supports PDFs, DOCX, TXT, and CSV.

![logo](assets/logo.png)

## ðŸ“‹ Table of Contents

- [ðŸ“Œ Features](#-features)
- [ðŸš€ Demo](#-demo)
- [ðŸ› ï¸ Installation](#ï¸-installation)
- [ðŸ“‚ Supported File Types](#-supported-file-types)
- [ðŸ’» Usage](#-usage)
- [ðŸ“¦ One-Click Desktop App (Windows)](#-one-click-desktop-app-windows)
- [ðŸ³ Docker Support](#-docker-support)
- [âš™ï¸ Project Structure](#ï¸-project-structure)
- [ðŸ§  Future Upgrades](#-future-upgrades)
- [ðŸ§‘â€ðŸ’» Credits](#-credits)

## ðŸ“Œ Features

âœ… Fully offline document assistant  
âœ… Multi-format support: PDF, DOCX, TXT, CSV  
âœ… Smart chunk-based retrieval with FAISS  
âœ… Uses Ollama with local models (`mistral`, `llama2`, etc.)  
âœ… Per-session memory using LangChain  
âœ… Auto-summarizes uploaded files  
âœ… Shows matched chunks used for each answer  
âœ… Suggests follow-up questions using LLM  
âœ… Exports full Q&A session as `.txt` or `.json`  
âœ… Optional `.exe` for one-click launch  
âœ… Optional Docker container support  
âœ… Friendly Gradio UI with PDF previews  

## ðŸš€ Demo

![demo-screenshot](assets/demo.png)  
> UI includes file upload, chat interface, context highlighting, suggestions, and export buttons.

## ðŸ› ï¸ Installation

### âœ… Prerequisites

- Python 3.10+
- Ollama installed & running locally (https://ollama.com/)
- Models downloaded (e.g., `mistral`, `llama2`):

```bash
ollama run mistral
```

### ðŸ§° Step-by-Step Setup

1. **Clone this repo:**

```bash
git clone https://github.com/your-username/pdf-ai-assistant.git
cd pdf-ai-assistant
```

2. **Install dependencies:**

```bash
pip install -r requirements.txt
```

3. **Run the app:**

```bash
python ui_gradio.py
```

## ðŸ“‚ Supported File Types

| Type | Support |
|------|---------|
| `.pdf` | âœ… (with preview) |
| `.docx` | âœ… |
| `.txt` | âœ… |
| `.csv` | âœ… |

## ðŸ’» Usage

1. Upload one or more supported files  
2. Summaries are auto-generated  
3. Ask questions in chat  
4. View answer + matched chunks + follow-up suggestions  
5. Export Q&A session as `.txt` or `.json`

## ðŸ“¦ One-Click Desktop App (Windows)

1. Build executable with:

```bash
pyinstaller --onefile --windowed --icon=icon.ico main.py
```

2. Run `dist/main.exe`

âœ… No Python or terminal needed  
âœ… Browser auto-launch or native window (via `pywebview`)

## ðŸ³ Docker Support

### ðŸ“¥ Build image:

```bash
docker build -t offline-pdf-assistant .
```

### ðŸš€ Run:

```bash
docker run -it --rm -p 7860:7860 offline-pdf-assistant
```

Ensure Ollama is running on the host, not inside the container.

## âš™ï¸ Project Structure

```
.
â”œâ”€â”€ main.py                 # Launcher (for .exe)
â”œâ”€â”€ ui_gradio.py           # Main Gradio interface
â”œâ”€â”€ qa_engine.py           # LLM logic, memory, prompts
â”œâ”€â”€ embedder.py            # SentenceTransformer embedding
â”œâ”€â”€ retriever.py           # FAISS-based semantic search
â”œâ”€â”€ pdf_parser.py          # Multi-format document reader
â”œâ”€â”€ assets/
â”‚   â”œâ”€â”€ logo.png           # Logo for UI
â”‚   â””â”€â”€ icon.ico           # App icon for .exe
â”œâ”€â”€ requirements.txt
â””â”€â”€ Dockerfile
```

## ðŸ§  Future Upgrades

- [ ] Highlight chunk directly in document view  
- [ ] Add user login / session-based dashboards  
- [ ] Export full conversation as PDF  
- [ ] Voice input + text-to-speech replies  
- [ ] Cloud sync (opt-in)

## ðŸ§‘â€ðŸ’» Credits

Made with ðŸ§  by **Aaryaksh Bhatnagar**  
B.Tech AI & DS â€“ Arya College of Engineering & IT  
> Portfolio-ready AI product built from scratch for private LLM workflows
